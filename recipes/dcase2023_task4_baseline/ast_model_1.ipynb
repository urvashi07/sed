{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, ASTConfig, ASTModel, AutoFeatureExtractor, ASTForAudioClassification, TrainingArguments, Trainer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "import yaml\n",
    "import torchaudio\n",
    "import os\n",
    "\n",
    "from transformers import AdamW\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint, device_map='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "labels2id = OrderedDict(\n",
    "    {\n",
    "        \"Alarm_bell_ringing\": 0,\n",
    "        \"Blender\": 1,\n",
    "        \"Cat\": 2,\n",
    "        \"Dishes\": 3,\n",
    "        \"Dog\": 4,\n",
    "        \"Electric_shaver_toothbrush\": 5,\n",
    "        \"Frying\": 6,\n",
    "        \"Running_water\": 7,\n",
    "        \"Speech\": 8,\n",
    "        \"Vacuum_cleaner\": 9,\n",
    "    }\n",
    ")\n",
    "\n",
    "id2labels = {value: key for key, value in labels2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.config.labels2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./confs/default.yaml\", \"r\") as f:\n",
    "        configs = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "SAMPLE_RATE = configs[\"data\"][\"fs\"]\n",
    "N_FFT = configs[\"feats\"][\"n_window\"]\n",
    "WIN_LENGTH = configs[\"feats\"][\"n_window\"]\n",
    "HOP_LENGTH = configs[\"feats\"][\"hop_length\"]\n",
    "F_MIN = configs[\"feats\"][\"f_min\"]\n",
    "F_MAX = configs[\"feats\"][\"f_max\"]\n",
    "N_MELS = configs[\"feats\"][\"n_mels\"]\n",
    "WINDOW_FN = torch.hamming_window\n",
    "WKWARGS = {\"periodic\": False}\n",
    "POWER = 1\n",
    "NUM_SAMPLES = SAMPLE_RATE\n",
    "\n",
    "config = ASTConfig(\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    hidden_act='gelu',\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-12,\n",
    "    patch_size=16,\n",
    "    qkv_bias=True,\n",
    "    frequency_stride=10,\n",
    "    time_stride=10,\n",
    "    max_length=1024,\n",
    "    num_labels=len(labels2id)  # Number of sound event classes\n",
    ")\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate = SAMPLE_RATE,\n",
    "        n_fft = N_FFT,\n",
    "        hop_length= HOP_LENGTH,\n",
    "        n_mels = N_MELS,\n",
    "        win_length= WIN_LENGTH,\n",
    "        f_min=F_MIN,\n",
    "        f_max=F_MAX,\n",
    "        window_fn=WINDOW_FN,\n",
    "        wkwargs=WKWARGS,\n",
    "        power=POWER\n",
    "    )\n",
    "\n",
    "config.num_labels = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(example, audio_file_dir, num_samples, transformation, labels2id, target_sample_rate=16000):\n",
    "    # This function will do what your __getitem__ method does\n",
    "    \n",
    "    # Load audio\n",
    "    signal, sr = torchaudio.load(os.path.join(audio_file_dir, example['filename']))\n",
    "    label = example[\"event_label\"]\n",
    "    label_int = labels2id[label]\n",
    "    # Resample if necessary\n",
    "    resampler = torchaudio.transforms.Resample(sr, target_sample_rate)\n",
    "    signal = resampler(signal)\n",
    "    \n",
    "    # Mix down if necessary\n",
    "    if signal.shape[0] > 1:\n",
    "        signal = torch.mean(signal, dim = 0, keepdim = True)\n",
    "    \n",
    "    # Cut if necessary\n",
    "    onset_frame = int(example['onset'] * target_sample_rate)\n",
    "    offset_frame = int(example['offset'] * target_sample_rate)\n",
    "    signal = signal[:, onset_frame:offset_frame]\n",
    "    if signal.shape[1] > num_samples:\n",
    "        signal = signal[:, :num_samples]\n",
    "    \n",
    "    # Pad if necessary\n",
    "    length_signal = signal.shape[1]\n",
    "    if length_signal < num_samples:\n",
    "        num_missing_samples = num_samples - length_signal\n",
    "        last_dim_padding = (0, num_missing_samples)\n",
    "        signal = nn.functional.pad(signal, last_dim_padding)\n",
    "\n",
    "    signal = signal.view(-1)\n",
    "    #print(signal.shape)\n",
    "    #print(\"#################\")\n",
    "\n",
    "    times = torch.tensor([onset_frame, offset_frame], dtype=torch.float32)\n",
    "    # Apply transformation\n",
    "    #signal = transformation(signal)\n",
    "    #print(signal.shape)\n",
    "\n",
    "    # Reshape and permute the input tensor\n",
    "    #signal = signal.squeeze(0).to_dense().permute(2, 0, 1)\n",
    "    \n",
    "    example['input_values'] = np.array(signal)\n",
    "    #example['input_values'] = torch.tensor(example['input_values'])\n",
    "    example['label'] = torch.tensor(label_int)\n",
    "    #example['label_int'] = torch.tensor(example['label_int'])\n",
    "    #example['times'] = times\n",
    "    #example['times'] = torch.tensor(example['times'])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_demo (/home/unegi/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_demo/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "\n",
    "dataset = dataset.sort(\"id\")\n",
    "\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93680,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"audio\"][\"array\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/unegi/.cache/huggingface/datasets/csv/default-03475b778f293dce/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca43bff3849c412b991c605c9658cb57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_files = {\"train\": configs[\"data\"][\"synth_tsv\"], \"test\": configs[\"data\"][\"synth_val_tsv\"]}\n",
    "trial_dataset = load_dataset(\"csv\", data_files=data_files, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93680,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"audio\"][\"array\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/home/unegi/.cache/huggingface/datasets/downloads/extracted/b1e597323d8b9a7257310b4aaaba1bc74facde05d42c9048752c990cbbd1d77b/dev_clean/1272/128104/1272-128104-0000.flac',\n",
       " 'array': array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00042725, 0.00057983,\n",
       "        0.0010376 ]),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/unegi/.cache/huggingface/datasets/csv/default-03475b778f293dce/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-6048fb7f002aacf1.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = trial_dataset[\"train\"].map(process_data, \n",
    "                                           fn_kwargs={'audio_file_dir': configs[\"data\"][\"synth_folder\"], \n",
    "                                                      'num_samples': NUM_SAMPLES,\n",
    "                                                       'transformation': mel_spectrogram,\n",
    "                                                        'labels2id': labels2id,\n",
    "                                                        'target_sample_rate':16000, \n",
    "                                                        })\n",
    "\n",
    "columns_to_remove = [\"onset\", \"offset\", \"event_label\"]\n",
    "\n",
    "train_dataset = train_dataset.remove_columns(columns_to_remove) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/unegi/.cache/huggingface/datasets/csv/default-03475b778f293dce/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-305c56cf965f6ddf.arrow\n"
     ]
    }
   ],
   "source": [
    "test_dataset = trial_dataset[\"test\"].map(process_data, \n",
    "                                           fn_kwargs={'audio_file_dir': configs[\"data\"][\"synth_val_folder\"], \n",
    "                                                      'num_samples': NUM_SAMPLES,\n",
    "                                                       'transformation': mel_spectrogram,\n",
    "                                                        'labels2id': labels2id,\n",
    "                                                        'target_sample_rate':16000, \n",
    "                                                        })\n",
    "test_dataset = test_dataset.remove_columns(columns_to_remove) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_partial = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check if everything works\n",
    "if use_partial:\n",
    "    partial_dataset = train_dataset.select(range(2))\n",
    "    partial_eval_dataset = test_dataset.select(range(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/unegi/.cache/huggingface/datasets/csv/default-03475b778f293dce/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0197f77b7d74d9a4.arrow\n",
      "Loading cached processed dataset at /home/unegi/.cache/huggingface/datasets/csv/default-03475b778f293dce/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-54c309f33e818951.arrow\n"
     ]
    }
   ],
   "source": [
    "def feature_function(examples):\n",
    "    return feature_extractor(examples[\"input_values\"],  sampling_rate=16000, return_tensors=\"pt\")\n",
    "\n",
    "tokenized_train_datasets = partial_dataset.map(feature_function, batched=True)\n",
    "tokenized_val_datasets = partial_eval_dataset.map(feature_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_train_datasets = tokenized_train_datasets.remove_columns([\"filename\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_val_datasets = tokenized_val_datasets.remove_columns([\"filename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['filename', 'input_values', 'label'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38676/3043712826.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_train_datasets.rename_column(\"label\", \"labels\")\n",
    "valid_dataset = tokenized_val_datasets.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['filename', 'input_values', 'labels'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=2,              # total number of training epochs\n",
    "    per_device_train_batch_size=4,  # batch size per device during training\n",
    "    per_device_eval_batch_size=4,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:19:33] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 16:19:33] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 16:19:33] No GPU found.\n",
      "[codecarbon INFO @ 16:19:33] [setup] CPU Tracking...\n",
      "[codecarbon ERROR @ 16:19:33] Unable to read Intel RAPL files for CPU power, we will use a constant for your CPU power. Please view https://github.com/mlco2/codecarbon/issues/244 for workarounds : [Errno 13] Permission denied: '/sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj'\n",
      "[codecarbon INFO @ 16:19:33] Tracking Intel CPU via RAPL interface\n",
      "[codecarbon ERROR @ 16:19:35] Unable to read Intel RAPL files for CPU power, we will use a constant for your CPU power. Please view https://github.com/mlco2/codecarbon/issues/244 for workarounds : [Errno 13] Permission denied: '/sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj'\n",
      "[codecarbon INFO @ 16:19:35] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 16:19:35]   Platform system: Linux-5.19.0-43-generic-x86_64-with-glibc2.10\n",
      "[codecarbon INFO @ 16:19:35]   Python version: 3.8.5\n",
      "[codecarbon INFO @ 16:19:35]   Available RAM : 7.623 GB\n",
      "[codecarbon INFO @ 16:19:35]   CPU count: 8\n",
      "[codecarbon INFO @ 16:19:35]   CPU model: Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz\n",
      "[codecarbon INFO @ 16:19:35]   GPU count: None\n",
      "[codecarbon INFO @ 16:19:35]   GPU model: None\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=train_dataset, eval_dataset=valid_dataset, compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unegi/anaconda3/envs/dcase2023/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd835c5079a74e13978175374c31aec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:19:53] Energy consumed for RAM : 0.000012 kWh. RAM Power : 2.858745574951172 W\n",
      "[codecarbon INFO @ 16:19:54] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 16:19:54] 0.000012 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 16:20:08] Energy consumed for RAM : 0.000024 kWh. RAM Power : 2.858745574951172 W\n",
      "[codecarbon INFO @ 16:20:08] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 16:20:08] 0.000024 kWh of electricity used since the begining.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e3521ab4e84f23ae80d331dfb53fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.696477890014648, 'eval_accuracy': 0.0, 'eval_runtime': 3.82, 'eval_samples_per_second': 0.262, 'eval_steps_per_second': 0.262, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:20:23] Energy consumed for RAM : 0.000036 kWh. RAM Power : 2.858745574951172 W\n",
      "[codecarbon INFO @ 16:20:23] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 16:20:23] 0.000036 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 16:20:38] Energy consumed for RAM : 0.000047 kWh. RAM Power : 2.858745574951172 W\n",
      "[codecarbon INFO @ 16:20:39] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 16:20:39] 0.000047 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 16:20:53] Energy consumed for RAM : 0.000059 kWh. RAM Power : 2.858745574951172 W\n",
      "[codecarbon INFO @ 16:20:53] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 16:20:53] 0.000059 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 16:21:08] Energy consumed for RAM : 0.000071 kWh. RAM Power : 2.858745574951172 W\n",
      "[codecarbon INFO @ 16:21:08] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 16:21:08] 0.000071 kWh of electricity used since the begining.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19bee1f8665d4580b95a35d18398f9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:21:12] Energy consumed for RAM : 0.000074 kWh. RAM Power : 2.858745574951172 W\n",
      "[codecarbon INFO @ 16:21:12] Energy consumed for all CPUs : 0.000000 kWh. All CPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 16:21:12] 0.000074 kWh of electricity used since the begining.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.6900858879089355, 'eval_accuracy': 0.0, 'eval_runtime': 3.0676, 'eval_samples_per_second': 0.326, 'eval_steps_per_second': 0.326, 'epoch': 2.0}\n",
      "{'train_runtime': 93.5875, 'train_samples_per_second': 0.043, 'train_steps_per_second': 0.021, 'train_loss': 9.947755813598633, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2, training_loss=9.947755813598633, metrics={'train_runtime': 93.5875, 'train_samples_per_second': 0.043, 'train_steps_per_second': 0.021, 'train_loss': 9.947755813598633, 'epoch': 2.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b037e77ad3464188f6ed933ee88295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 6.6900858879089355,\n",
       " 'eval_accuracy': 0.0,\n",
       " 'eval_runtime': 3.1344,\n",
       " 'eval_samples_per_second': 0.319,\n",
       " 'eval_steps_per_second': 0.319,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['filename', 'input_values', 'labels']\n"
     ]
    }
   ],
   "source": [
    "# Get the training dataset\n",
    "training_dataset = trainer.train_dataset\n",
    "\n",
    "# Check the column names\n",
    "column_names = training_dataset.column_names\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcase2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "700379c3244b0a734b7815c110d6ccb11664b277ccff9788ce4e0f0e1a2f4efe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
