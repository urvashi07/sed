{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:59:04.242061: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-18 22:59:05.909684: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-18 22:59:05.914059: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-18 22:59:12.295417: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import audioread\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import librosa\n",
    "import librosa.display as display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from IPython.display import Audio\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "from catalyst.dl import Callback\n",
    "#from catalyst.runners import Runner\n",
    "from fastprogress import progress_bar\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "#from functools import partial\n",
    "from sklearn import metrics\n",
    "from datasets import PANNsDataset\n",
    "from panns_models import *\n",
    "from evaluate import Evaluator, StatisticsContainer\n",
    "from losses import PANNsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "    \n",
    "    \n",
    "def get_logger(out_file=None):\n",
    "    logger = logging.getLogger()\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    logger.handlers = []\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    if out_file is not None:\n",
    "        fh = logging.FileHandler(out_file)\n",
    "        fh.setFormatter(formatter)\n",
    "        fh.setLevel(logging.INFO)\n",
    "        logger.addHandler(fh)\n",
    "    logger.info(\"logger set up\")\n",
    "    return logger\n",
    "    \n",
    "    \n",
    "@contextmanager\n",
    "def timer(name: str, logger: Optional[logging.Logger] = None):\n",
    "    t0 = time.time()\n",
    "    msg = f\"[{name}] start\"\n",
    "    if logger is None:\n",
    "        print(msg)\n",
    "    else:\n",
    "        logger.info(msg)\n",
    "    yield\n",
    "\n",
    "    msg = f\"[{name}] done in {time.time() - t0:.2f} s\"\n",
    "    if logger is None:\n",
    "        print(msg)\n",
    "    else:\n",
    "        logger.info(msg)\n",
    "    \n",
    "    \n",
    "set_seed(1213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"./confs/default.yaml\", \"r\") as f:\n",
    "        configs = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROOT = Path.cwd().parent\n",
    "#INPUT_ROOT = ROOT / \"input\"\n",
    "#RAW_DATA = INPUT_ROOT / \"birdsong-recognition\"\n",
    "#TRAIN_AUDIO_DIR = RAW_DATA / \"train_audio\"\n",
    "TRAIN_DATA = configs[\"data\"][\"synth_tsv\"]\n",
    "TRAIN_RESAMPLED_AUDIO_DIRS = configs[\"data\"][\"synth_folder\"]\n",
    "TEST_AUDIO_DIR = configs[\"data\"][\"test_folder\"]\n",
    "train_df = pd.read_csv(TRAIN_DATA, sep = \"\\t\")\n",
    "\n",
    "VAL_DATA = configs[\"data\"][\"synth_val_tsv\"]\n",
    "VAL_AUDIO_DIR = configs[\"data\"][\"synth_val_folder\"]\n",
    "\n",
    "val_df = pd.read_csv(TRAIN_DATA, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_config = {\\n    \"sample_rate\": SAMPLE_RATE,\\n    \"window_size\": WIN_LENGTH,\\n    \"hop_size\": HOP_LENGTH,\\n    \"mel_bins\": N_MELS,\\n    \"fmin\": F_MIN,\\n    \"fmax\": F_MAX,\\n    \"classes_num\": 10\\n}'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_RATE = configs[\"data\"][\"fs\"]\n",
    "N_FFT = configs[\"feats\"][\"n_window\"]\n",
    "WIN_LENGTH = configs[\"feats\"][\"n_window\"]\n",
    "HOP_LENGTH = configs[\"feats\"][\"hop_length\"]\n",
    "F_MIN = configs[\"feats\"][\"f_min\"]\n",
    "F_MAX = configs[\"feats\"][\"f_max\"]\n",
    "N_MELS = configs[\"feats\"][\"n_mels\"]\n",
    "WINDOW_FN = torch.hamming_window\n",
    "WKWARGS = {\"periodic\": False}\n",
    "POWER = 1\n",
    "NUM_SAMPLES = SAMPLE_RATE\n",
    "\n",
    "LEARNING_RATE = configs[\"opt\"][\"lr\"]\n",
    "epochs = 5\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "    #frame_length_in_seconds\n",
    "frame_length_sec = HOP_LENGTH / SAMPLE_RATE\n",
    "\n",
    "\"\"\"model_config = {\n",
    "    \"sample_rate\": SAMPLE_RATE,\n",
    "    \"window_size\": WIN_LENGTH,\n",
    "    \"hop_size\": HOP_LENGTH,\n",
    "    \"mel_bins\": N_MELS,\n",
    "    \"fmin\": F_MIN,\n",
    "    \"fmax\": F_MAX,\n",
    "    \"classes_num\": 10\n",
    "}\"\"\"\n",
    "\n",
    "#model = PANNsCNN14Att(**model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_dataset = PANNsDataset(annotations_file = configs[\"data\"][\"synth_tsv\"], \n",
    "                                          audio_dir = configs[\"data\"][\"synth_folder\"], \n",
    "                                          transformation = None, \n",
    "                                          target_sample_rate = SAMPLE_RATE,\n",
    "                                          num_samples = NUM_SAMPLES,\n",
    "                                          device = device)\n",
    "\n",
    "val_dataset = PANNsDataset(annotations_file = configs[\"data\"][\"synth_val_tsv\"],\n",
    "                                          audio_dir = configs[\"data\"][\"synth_val_folder\"],\n",
    "                                          transformation = None, \n",
    "                                          target_sample_rate = SAMPLE_RATE,\n",
    "                                          num_samples = NUM_SAMPLES,\n",
    "                                          device = device)\n",
    "# loaders\n",
    "loaders = {\n",
    "    \"train\": DataLoader(train_dataset, \n",
    "                             batch_size= 2, \n",
    "                             shuffle=False),\n",
    "    \"valid\": DataLoader(val_dataset, \n",
    "                             batch_size=2, \n",
    "                             shuffle=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"waveform\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"sample_rate\": 16000,\n",
    "    \"window_size\": 1024,\n",
    "    \"hop_size\": 320,\n",
    "    \"mel_bins\": 64,\n",
    "    \"fmin\": 50,\n",
    "    \"fmax\": 14000,\n",
    "    \"classes_num\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"./log_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unegi/Documents/dcase-task4/DESED_task/recipes/dcase2023_task4_baseline/panns_models.py:293: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  self.melW = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels,\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model_config[\"classes_num\"] = 527\n",
    "model = PANNsCNN14Att(**model_config)\n",
    "weights = torch.load(\"Cnn14_DecisionLevelAtt_mAP0.425.pth\", map_location = \"cpu\")\n",
    "# Fixed in V3\n",
    "model.load_state_dict(weights[\"model\"])\n",
    "model.att_block = AttBlock(2048, 10, activation='sigmoid')\n",
    "#model.att_block.init_weights()\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# Loss\n",
    "criterion = PANNsLoss().to(device)\n",
    "#F1Callback(input_key=\"targets\", output_key=\"logits\", prefix=\"f1\")\n",
    "# callbacks\n",
    "#callbacks = [\n",
    "    \n",
    "#    mAPCallback(input_key=\"targets\", output_key=\"logits\", prefix=\"mAP\"),\n",
    " #   CheckpointCallback(save_best =0, logdir = logdir)\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16000])\n"
     ]
    }
   ],
   "source": [
    "for batch in loaders[\"train\"]:\n",
    "    print(batch[\"waveform\"].shape)    #print(b)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _lwlrap_sklearn(truth, scores):\n",
    "    \"\"\"Reference implementation from https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8\"\"\"\n",
    "    sample_weight = np.sum(truth > 0, axis=1)\n",
    "    nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n",
    "    overall_lwlrap = metrics.label_ranking_average_precision_score(\n",
    "        truth[nonzero_weight_sample_indices, :] > 0, \n",
    "        scores[nonzero_weight_sample_indices, :], \n",
    "        sample_weight=sample_weight[nonzero_weight_sample_indices])\n",
    "    return overall_lwlrap\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "class MetricMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.y_true = []\n",
    "        self.y_pred = []\n",
    "    \n",
    "    def update(self, y_true, y_pred):\n",
    "        self.y_true.extend(y_true.cpu().detach().numpy().tolist())\n",
    "        self.y_pred.extend(y_pred.cpu().detach().numpy().tolist())\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        #score_class, weight = lwlrap(np.array(self.y_true), np.array(self.y_pred))\n",
    "        self.score = _lwlrap_sklearn(np.array(self.y_true), np.array(self.y_pred)) #(score_class * weight).sum()\n",
    "        return {\n",
    "            \"lwlrap\" : self.score\n",
    "        }\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lwlrap = -np.inf\n",
    "early_stop_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_path = configs[\"data\"][\"statistics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(statistics_path):\n",
    "    os.mkdir(statistics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_container = StatisticsContainer(statistics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(device, model, loader, criterion, optimizer, scheduler, epoch):\n",
    "    losses = AverageMeter()\n",
    "    scores = MetricMeter()\n",
    "\n",
    "    model.train()\n",
    "    t = tqdm(loader)\n",
    "    for i, sample in enumerate(t):\n",
    "        optimizer.zero_grad()\n",
    "        input = sample['waveform'].to(device)\n",
    "        target = sample['targets'].to(device)\n",
    "        output = model(input)\n",
    "        #print(output)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #if scheduler and args.step_scheduler:\n",
    "            #scheduler.step()\n",
    "\n",
    "        bs = input.size(0)\n",
    "        scores.update(target, torch.sigmoid(torch.max(output['framewise_output'], dim=1)[0]))\n",
    "        losses.update(loss.item(), bs)\n",
    "\n",
    "        t.set_description(f\"Train E:{epoch} - Loss{losses.avg:0.4f}\")\n",
    "    t.close()\n",
    "    return scores.avg, losses.avg\n",
    "        \n",
    "def valid_epoch(device, model, loader, criterion, epoch):\n",
    "    \n",
    "    \n",
    "    logging.info('Validate bal mAP: {:.3f}'.format(\n",
    "                np.mean(eval_statistics['average_precision'])))\n",
    "    losses = AverageMeter()\n",
    "    scores = MetricMeter()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        t = tqdm(loader)\n",
    "        for i, sample in enumerate(t):\n",
    "            eval_statistics = evaluator.evaluate(sample)\n",
    "            statistics_container.append(i, eval_statistics, data_type='eval')\n",
    "            logging.info('Validate bal mAP: {:.3f}'.format(\n",
    "                np.mean(eval_statistics['average_precision'])))\n",
    "            print('Validate bal mAP: {:.3f}'.format(\n",
    "                np.mean(eval_statistics['average_precision'])))\n",
    "            input = sample['waveform'].to(device)\n",
    "            target = sample['targets'].to(device)\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            bs = input.size(0)\n",
    "            scores.update(target, torch.sigmoid(torch.max(output['framewise_output'], dim=1)[0]))\n",
    "            losses.update(loss.item(), bs)\n",
    "            t.set_description(f\"Valid E:{epoch} - Loss:{losses.avg:0.4f}\")\n",
    "    t.close()\n",
    "    return scores.avg, losses.avg\n",
    "\n",
    "def test_epoch(device, model, loader):\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    id_list = []\n",
    "    with torch.no_grad():\n",
    "        t = tqdm(loader)\n",
    "        for i, sample in enumerate(t):\n",
    "            input = sample[\"image\"].to(device)\n",
    "            bs, seq, w = input.shape\n",
    "            input = input.reshape(bs*seq, w)\n",
    "            id = sample[\"id\"]\n",
    "            output = model(input)\n",
    "            output = torch.sigmoid(torch.max(output['framewise_output'], dim=1)[0])\n",
    "            output = output.reshape(bs, seq, -1)\n",
    "            output = torch.sum(output, dim=1)\n",
    "            #output, _ = torch.max(output, dim=1)\n",
    "            output = output.cpu().detach().numpy().tolist()\n",
    "            pred_list.extend(output)\n",
    "            id_list.extend(id)\n",
    "    \n",
    "    return pred_list, id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = PANNsLoss()\n",
    "best_lwlrap = -np.inf\n",
    "early_stop_count = 0\n",
    "scheduler = None\n",
    "save_path = \"./log_dir\"\n",
    "exp_name = \"logging\"\n",
    "early_stop = 15\n",
    "\n",
    "for epoch in range(2):\n",
    "        train_avg, train_loss = train_epoch(device, model, loaders[\"train\"], criterion, optimizer, scheduler, epoch)\n",
    "        valid_avg, valid_loss = valid_epoch(device, model, loaders[\"valid\"], criterion, epoch)\n",
    "        \n",
    "        #if args.epoch_scheduler:\n",
    "           # scheduler.step()\n",
    "        \n",
    "        content = f\"\"\"\n",
    "                {time.ctime()} \\n\n",
    "                Epoch:{epoch}, lr:{optimizer.param_groups[0]['lr']:.7}\\n\n",
    "                Train Loss:{train_loss:0.4f} - LWLRAP:{train_avg['lwlrap']:0.4f}\\n\n",
    "                Valid Loss:{valid_loss:0.4f} - LWLRAP:{valid_avg['lwlrap']:0.4f}\\n\n",
    "        \"\"\"\n",
    "        with open(f'{save_path}/log_{exp_name}.txt', 'a') as appender:\n",
    "            appender.write(content+'\\n')\n",
    "        \n",
    "        if valid_avg['lwlrap'] > best_lwlrap:\n",
    "            print(f\"########## >>>>>>>> Model Improved From {best_lwlrap} ----> {valid_avg['lwlrap']}\")\n",
    "            torch.save(model.state_dict(), save_path+'.bin')\n",
    "            best_lwlrap = valid_avg['lwlrap']\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "        #torch.save(model.state_dict(), os.path.join(args.save_path, f'fold-{args.fold}_last.bin'))\n",
    "\n",
    "        if early_stop == early_stop_count:\n",
    "            print(\"\\n $$$ ---? Ohoo.... we reached early stoping count :\", early_stop_count)\n",
    "            break\n",
    "    \n",
    "model.load_state_dict(torch.load(save_path+'.bin'), map_location=device)\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"target_cols = sub_df.columns[1:].values.tolist()\n",
    "test_pred, ids = test_epoch(device, model, test_loader)\n",
    "print(np.array(test_pred).shape)\n",
    "\n",
    "test_pred_df = pd.DataFrame({\n",
    "        \"recording_id\" : sub_df.recording_id.values\n",
    "    })\n",
    "test_pred_df[target_cols] = test_pred\n",
    "test_pred_df.to_csv(save_path+'.bin'+\"-submission.csv\", index=False)\n",
    "print(os.path.join(save_path, f\"-submission.csv\"))\n",
    "        \n",
    "        #print(content)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = sub_df.columns[1:].values.tolist()\n",
    "test_pred, ids = test_epoch(device, model, test_loader)\n",
    "print(np.array(test_pred).shape)\n",
    "\n",
    "test_pred_df = pd.DataFrame({\n",
    "        \"recording_id\" : sub_df.recording_id.values\n",
    "    })\n",
    "test_pred_df[target_cols] = test_pred\n",
    "test_pred_df.to_csv(save_path+'.bin'+\"-submission.csv\", index=False)\n",
    "print(os.path.join(save_path, f\"-submission.csv\"))\n",
    "        \n",
    "        #print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"warnings.simplefilter(\"ignore\")\n",
    "\n",
    "runner = SupervisedRunner(\n",
    "    input_key=\"waveform\",\n",
    "    target_key=\"targets\")\n",
    "\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    loaders=loaders,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    "    logdir=f\"fold0\",\n",
    "    callbacks=callbacks)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcase2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "700379c3244b0a734b7815c110d6ccb11664b277ccff9788ce4e0f0e1a2f4efe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
