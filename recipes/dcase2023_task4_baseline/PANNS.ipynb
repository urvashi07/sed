{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import audioread\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import librosa\n",
    "import librosa.display as display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from IPython.display import Audio\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "from catalyst.dl import SupervisedRunner, CallbackOrder, Callback, CheckpointCallback\n",
    "from catalyst.runners import Runner\n",
    "from fastprogress import progress_bar\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "#from functools import partial\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "    \n",
    "    \n",
    "def get_logger(out_file=None):\n",
    "    logger = logging.getLogger()\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    logger.handlers = []\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    if out_file is not None:\n",
    "        fh = logging.FileHandler(out_file)\n",
    "        fh.setFormatter(formatter)\n",
    "        fh.setLevel(logging.INFO)\n",
    "        logger.addHandler(fh)\n",
    "    logger.info(\"logger set up\")\n",
    "    return logger\n",
    "    \n",
    "    \n",
    "@contextmanager\n",
    "def timer(name: str, logger: Optional[logging.Logger] = None):\n",
    "    t0 = time.time()\n",
    "    msg = f\"[{name}] start\"\n",
    "    if logger is None:\n",
    "        print(msg)\n",
    "    else:\n",
    "        logger.info(msg)\n",
    "    yield\n",
    "\n",
    "    msg = f\"[{name}] done in {time.time() - t0:.2f} s\"\n",
    "    if logger is None:\n",
    "        print(msg)\n",
    "    else:\n",
    "        logger.info(msg)\n",
    "    \n",
    "    \n",
    "set_seed(1213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"./confs/default.yaml\", \"r\") as f:\n",
    "        configs = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'synth_folder': '../../data/dcase/dataset/dcase_synth/audio/train/synthetic21_train/soundscapes_16k/',\n",
       " 'synth_folder_44k': '../../data/dcase/dataset/dcase_synth/audio/train/synthetic21_train/soundscapes/',\n",
       " 'synth_tsv': '../../data/dcase/dataset/dcase_synth/metadata/train/synthetic21_train/soundscapes.tsv',\n",
       " 'strong_folder': '../../data/dcase/dataset/audio/train/strong_label_real_16k/',\n",
       " 'strong_folder_44k': '../../data/dcase/dataset/audio/train/strong_label_real/',\n",
       " 'strong_tsv': '../../data/dcase/dataset/metadata/train/audioset_strong.tsv',\n",
       " 'weak_folder': '../../data/dcase/dataset/audio/train/weak_16k/',\n",
       " 'weak_folder_44k': '../../data/dcase/dataset/audio/train/weak/',\n",
       " 'weak_tsv': '../../data/dcase/dataset/metadata/train/weak.tsv',\n",
       " 'unlabeled_folder': '../../data/dcase/dataset/audio/train/unlabel_in_domain_16k/',\n",
       " 'unlabeled_folder_44k': '../../data/dcase/dataset/audio/train/unlabel_in_domain/',\n",
       " 'synth_val_folder': '../../data/dcase/dataset/audio/validation/synthetic21_validation/soundscapes_16k/',\n",
       " 'synth_val_folder_44k': '../../data/dcase/dataset/dcase_synth/audio/validation/synthetic21_validation/soundscapes/',\n",
       " 'synth_val_tsv': '../../data/dcase/dataset/dcase_synth/metadata/validation/synthetic21_validation/soundscapes.tsv',\n",
       " 'synth_val_dur': '../../data/dcase/dataset/dcase_synth/metadata/validation/synthetic21_validation/durations.tsv',\n",
       " 'test_folder': '../../data/dcase/dataset/audio/validation/validation_16k/',\n",
       " 'test_folder_44k': '../../data/dcase/dataset/audio/validation/validation/',\n",
       " 'test_tsv': '../../data/dcase/dataset/metadata/validation/validation.tsv',\n",
       " 'test_dur': '../../data/dcase/dataset/metadata/validation/validation_durations.tsv',\n",
       " 'eval_folder': '../../data/dcase/dataset/audio/eval21_16k',\n",
       " 'eval_folder_44k': '../../data/dcase/dataset/audio/eval21',\n",
       " 'audio_max_len': 10,\n",
       " 'fs': 16000,\n",
       " 'net_subsample': 4}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROOT = Path.cwd().parent\n",
    "#INPUT_ROOT = ROOT / \"input\"\n",
    "#RAW_DATA = INPUT_ROOT / \"birdsong-recognition\"\n",
    "#TRAIN_AUDIO_DIR = RAW_DATA / \"train_audio\"\n",
    "TRAIN_DATA = configs[\"data\"][\"synth_tsv\"]\n",
    "TRAIN_RESAMPLED_AUDIO_DIRS = configs[\"data\"][\"synth_folder\"]\n",
    "TEST_AUDIO_DIR = configs[\"data\"][\"test_folder\"]\n",
    "train_df = pd.read_csv(TRAIN_DATA, sep = \"\\t\")\n",
    "\n",
    "VAL_DATA = configs[\"data\"][\"synth_val_tsv\"]\n",
    "VAL_AUDIO_DIR = configs[\"data\"][\"synth_val_folder\"]\n",
    "\n",
    "val_df = pd.read_csv(TRAIN_DATA, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def interpolate(x: torch.Tensor, ratio: int):\n",
    "    \"\"\"Interpolate data in time domain. This is used to compensate the\n",
    "    resolution reduction in downsampling of a CNN.\n",
    "\n",
    "    Args:\n",
    "      x: (batch_size, time_steps, classes_num)\n",
    "      ratio: int, ratio to interpolate\n",
    "    Returns:\n",
    "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
    "    \"\"\"\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n",
    "    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n",
    "    is the same as the value of the last frame.\n",
    "    Args:\n",
    "      framewise_output: (batch_size, frames_num, classes_num)\n",
    "      frames_num: int, number of frames to pad\n",
    "    Outputs:\n",
    "      output: (batch_size, frames_num, classes_num)\n",
    "    \"\"\"\n",
    "    pad = framewise_output[:, -1:, :].repeat(\n",
    "        1, frames_num - framewise_output.shape[1], 1)\n",
    "    \"\"\"tensor for padding\"\"\"\n",
    "\n",
    "    output = torch.cat((framewise_output, pad), dim=1)\n",
    "    \"\"\"(batch_size, frames_num, classes_num)\"\"\"\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "            bias=False)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "            bias=False)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "\n",
    "    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n",
    "\n",
    "        x = input\n",
    "        x = F.relu_(self.bn1(self.conv1(x)))\n",
    "        x = F.relu_(self.bn2(self.conv2(x)))\n",
    "        if pool_type == 'max':\n",
    "            x = F.max_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg':\n",
    "            x = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg+max':\n",
    "            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "            x2 = F.max_pool2d(x, kernel_size=pool_size)\n",
    "            x = x1 + x2\n",
    "        else:\n",
    "            raise Exception('Incorrect argument!')\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\",\n",
    "                 temperature=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.temperature = temperature\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.bn_att = nn.BatchNorm1d(out_features)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "        init_bn(self.bn_att)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFTBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Base class for DFT and IDFT matrix\"\"\"\n",
    "        super(DFTBase, self).__init__()\n",
    "\n",
    "    def dft_matrix(self, n):\n",
    "        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n",
    "        omega = np.exp(-2 * np.pi * 1j / n)\n",
    "        W = np.power(omega, x * y)\n",
    "        return W\n",
    "\n",
    "    def idft_matrix(self, n):\n",
    "        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n",
    "        omega = np.exp(2 * np.pi * 1j / n)\n",
    "        W = np.power(omega, x * y)\n",
    "        return W\n",
    "    \n",
    "    \n",
    "class STFT(DFTBase):\n",
    "    def __init__(self, n_fft=2048, hop_length=None, win_length=None, \n",
    "        window='hann', center=True, pad_mode='reflect', freeze_parameters=True):\n",
    "        \"\"\"Implementation of STFT with Conv1d. The function has the same output \n",
    "        of librosa.core.stft\n",
    "        \"\"\"\n",
    "        super(STFT, self).__init__()\n",
    "\n",
    "        assert pad_mode in ['constant', 'reflect']\n",
    "\n",
    "        self.n_fft = n_fft\n",
    "        self.center = center\n",
    "        self.pad_mode = pad_mode\n",
    "\n",
    "        # By default, use the entire frame\n",
    "        if win_length is None:\n",
    "            win_length = n_fft\n",
    "\n",
    "        # Set the default hop, if it's not already specified\n",
    "        if hop_length is None:\n",
    "            hop_length = int(win_length // 4)\n",
    "\n",
    "        fft_window = librosa.filters.get_window(window, win_length, fftbins=True)\n",
    "\n",
    "        # Pad the window out to n_fft size\n",
    "        fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
    "\n",
    "        # DFT & IDFT matrix\n",
    "        self.W = self.dft_matrix(n_fft)\n",
    "\n",
    "        out_channels = n_fft // 2 + 1\n",
    "\n",
    "        self.conv_real = nn.Conv1d(in_channels=1, out_channels=out_channels, \n",
    "            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1, \n",
    "            groups=1, bias=False)\n",
    "\n",
    "        self.conv_imag = nn.Conv1d(in_channels=1, out_channels=out_channels, \n",
    "            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1, \n",
    "            groups=1, bias=False)\n",
    "\n",
    "        self.conv_real.weight.data = torch.Tensor(\n",
    "            np.real(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n",
    "        # (n_fft // 2 + 1, 1, n_fft)\n",
    "\n",
    "        self.conv_imag.weight.data = torch.Tensor(\n",
    "            np.imag(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n",
    "        # (n_fft // 2 + 1, 1, n_fft)\n",
    "\n",
    "        if freeze_parameters:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"input: (batch_size, data_length)\n",
    "        Returns:\n",
    "          real: (batch_size, n_fft // 2 + 1, time_steps)\n",
    "          imag: (batch_size, n_fft // 2 + 1, time_steps)\n",
    "        \"\"\"\n",
    "\n",
    "        x = input[:, None, :]   # (batch_size, channels_num, data_length)\n",
    "\n",
    "        if self.center:\n",
    "            x = F.pad(x, pad=(self.n_fft // 2, self.n_fft // 2), mode=self.pad_mode)\n",
    "        #print(x.size())\n",
    "        real = self.conv_real(x)\n",
    "        imag = self.conv_imag(x)\n",
    "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        real = real[:, None, :, :].transpose(2, 3)\n",
    "        imag = imag[:, None, :, :].transpose(2, 3)\n",
    "        # (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "\n",
    "        return real, imag\n",
    "    \n",
    "    \n",
    "class Spectrogram(nn.Module):\n",
    "    def __init__(self, n_fft=2048, hop_length=None, win_length=None, \n",
    "        window='hann', center=True, pad_mode='reflect', power=2.0, \n",
    "        freeze_parameters=True):\n",
    "        \"\"\"Calculate spectrogram using pytorch. The STFT is implemented with \n",
    "        Conv1d. The function has the same output of librosa.core.stft\n",
    "        \"\"\"\n",
    "        super(Spectrogram, self).__init__()\n",
    "\n",
    "        self.power = power\n",
    "\n",
    "        self.stft = STFT(n_fft=n_fft, hop_length=hop_length, \n",
    "            win_length=win_length, window=window, center=center, \n",
    "            pad_mode=pad_mode, freeze_parameters=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"input: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "        Returns:\n",
    "          spectrogram: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "        \"\"\"\n",
    "\n",
    "        (real, imag) = self.stft.forward(input)\n",
    "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        spectrogram = real ** 2 + imag ** 2\n",
    "\n",
    "        if self.power == 2.0:\n",
    "            pass\n",
    "        else:\n",
    "            spectrogram = spectrogram ** (power / 2.0)\n",
    "\n",
    "        return spectrogram\n",
    "\n",
    "    \n",
    "class LogmelFilterBank(nn.Module):\n",
    "    def __init__(self, sr=32000, n_fft=2048, n_mels=64, fmin=50, fmax=14000, is_log=True, \n",
    "        ref=1.0, amin=1e-10, top_db=80.0, freeze_parameters=True):\n",
    "        \"\"\"Calculate logmel spectrogram using pytorch. The mel filter bank is \n",
    "        the pytorch implementation of as librosa.filters.mel \n",
    "        \"\"\"\n",
    "        super(LogmelFilterBank, self).__init__()\n",
    "\n",
    "        self.is_log = is_log\n",
    "        self.ref = ref\n",
    "        self.amin = amin\n",
    "        self.top_db = top_db\n",
    "\n",
    "        self.melW = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels,\n",
    "            fmin=fmin, fmax=fmax).T\n",
    "        # (n_fft // 2 + 1, mel_bins)\n",
    "\n",
    "        self.melW = nn.Parameter(torch.Tensor(self.melW))\n",
    "\n",
    "        if freeze_parameters:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"input: (batch_size, channels, time_steps)\n",
    "        \n",
    "        Output: (batch_size, time_steps, mel_bins)\n",
    "        \"\"\"\n",
    "\n",
    "        # Mel spectrogram\n",
    "        mel_spectrogram = torch.matmul(input, self.melW)\n",
    "\n",
    "        # Logmel spectrogram\n",
    "        if self.is_log:\n",
    "            output = self.power_to_db(mel_spectrogram)\n",
    "        else:\n",
    "            output = mel_spectrogram\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def power_to_db(self, input):\n",
    "        \"\"\"Power to db, this function is the pytorch implementation of \n",
    "        librosa.core.power_to_lb\n",
    "        \"\"\"\n",
    "        ref_value = self.ref\n",
    "        log_spec = 10.0 * torch.log10(torch.clamp(input, min=self.amin, max=np.inf))\n",
    "        log_spec -= 10.0 * np.log10(np.maximum(self.amin, ref_value))\n",
    "\n",
    "        if self.top_db is not None:\n",
    "            if self.top_db < 0:\n",
    "                raise ParameterError('top_db must be non-negative')\n",
    "            log_spec = torch.clamp(log_spec, min=log_spec.max().item() - self.top_db, max=np.inf)\n",
    "\n",
    "        return log_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropStripes(nn.Module):\n",
    "    def __init__(self, dim, drop_width, stripes_num):\n",
    "        \"\"\"Drop stripes. \n",
    "        Args:\n",
    "          dim: int, dimension along which to drop\n",
    "          drop_width: int, maximum width of stripes to drop\n",
    "          stripes_num: int, how many stripes to drop\n",
    "        \"\"\"\n",
    "        super(DropStripes, self).__init__()\n",
    "\n",
    "        assert dim in [2, 3]    # dim 2: time; dim 3: frequency\n",
    "\n",
    "        self.dim = dim\n",
    "        self.drop_width = drop_width\n",
    "        self.stripes_num = stripes_num\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"input: (batch_size, channels, time_steps, freq_bins)\"\"\"\n",
    "\n",
    "        assert input.ndimension() == 4\n",
    "\n",
    "        if self.training is False:\n",
    "            return input\n",
    "\n",
    "        else:\n",
    "            batch_size = input.shape[0]\n",
    "            total_width = input.shape[self.dim]\n",
    "\n",
    "            for n in range(batch_size):\n",
    "                self.transform_slice(input[n], total_width)\n",
    "\n",
    "            return input\n",
    "\n",
    "\n",
    "    def transform_slice(self, e, total_width):\n",
    "        \"\"\"e: (channels, time_steps, freq_bins)\"\"\"\n",
    "\n",
    "        for _ in range(self.stripes_num):\n",
    "            distance = torch.randint(low=0, high=self.drop_width, size=(1,))[0]\n",
    "            bgn = torch.randint(low=total_width - distance - 1, high=total_width, size=(1,))[0]\n",
    "\n",
    "            if self.dim == 2:\n",
    "                e[:, bgn : bgn + distance, :] = 0\n",
    "            else:\n",
    "                e[:, :, bgn : bgn + distance] = 0\n",
    "\n",
    "\n",
    "class SpecAugmentation(nn.Module):\n",
    "    def __init__(self, time_drop_width, time_stripes_num, freq_drop_width, \n",
    "        freq_stripes_num):\n",
    "        \"\"\"Spec augmetation. \n",
    "        [ref] Park, D.S., Chan, W., Zhang, Y., Chiu, C.C., Zoph, B., Cubuk, E.D. \n",
    "        and Le, Q.V., 2019. Specaugment: A simple data augmentation method \n",
    "        for automatic speech recognition. arXiv preprint arXiv:1904.08779.\n",
    "        Args:\n",
    "          time_drop_width: int\n",
    "          time_stripes_num: int\n",
    "          freq_drop_width: int\n",
    "          freq_stripes_num: int\n",
    "        \"\"\"\n",
    "\n",
    "        super(SpecAugmentation, self).__init__()\n",
    "\n",
    "        self.time_dropper = DropStripes(dim=2, drop_width=time_drop_width, \n",
    "            stripes_num=time_stripes_num)\n",
    "\n",
    "        self.freq_dropper = DropStripes(dim=3, drop_width=freq_drop_width, \n",
    "            stripes_num=freq_stripes_num)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.time_dropper(input)\n",
    "        x = self.freq_dropper(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PANNsCNN14Att(nn.Module):\n",
    "    def __init__(self, sample_rate: int, window_size: int, hop_size: int,\n",
    "                 mel_bins: int, fmin: int, fmax: int, classes_num: int):\n",
    "        super().__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "        self.interpolate_ratio = 32  # Downsampled ratio\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(\n",
    "            n_fft=window_size,\n",
    "            hop_length=hop_size,\n",
    "            win_length=window_size,\n",
    "            window=window,\n",
    "            center=center,\n",
    "            pad_mode=pad_mode,\n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(\n",
    "            sr=sample_rate,\n",
    "            n_fft=window_size,\n",
    "            n_mels=mel_bins,\n",
    "            fmin=fmin,\n",
    "            fmax=fmax,\n",
    "            ref=ref,\n",
    "            amin=amin,\n",
    "            top_db=top_db,\n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(\n",
    "            time_drop_width=64,\n",
    "            time_stripes_num=2,\n",
    "            freq_drop_width=8,\n",
    "            freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(mel_bins)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.att_block = AttBlock(2048, classes_num, activation='sigmoid')\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        \n",
    "    def cnn_feature_extractor(self, x):\n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        return x\n",
    "    \n",
    "    def preprocess(self, input, mixup_lambda=None):\n",
    "        # t1 = time.time()\n",
    "        #print(input.size())\n",
    "        #print\n",
    "        x = self.spectrogram_extractor(input)  # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)  # (batch_size, 1, time_steps, mel_bins)\n",
    "\n",
    "        frames_num = x.shape[2]\n",
    "\n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        return x, frames_num\n",
    "        \n",
    "\n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "        x, frames_num = self.preprocess(input, mixup_lambda=mixup_lambda)\n",
    "\n",
    "        # Output shape (batch size, channels, time, frequency)\n",
    "        x = self.cnn_feature_extractor(x)\n",
    "        \n",
    "        # Aggregate in frequency axis\n",
    "        x = torch.mean(x, dim=3)\n",
    "\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        # Get framewise output\n",
    "        framewise_output = interpolate(segmentwise_output,\n",
    "                                       self.interpolate_ratio)\n",
    "        framewise_output = pad_framewise_output(framewise_output, frames_num)\n",
    "\n",
    "        output_dict = {\n",
    "            'framewise_output': framewise_output,\n",
    "            'clipwise_output': clipwise_output\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_config = {\\n    \"sample_rate\": SAMPLE_RATE,\\n    \"window_size\": WIN_LENGTH,\\n    \"hop_size\": HOP_LENGTH,\\n    \"mel_bins\": N_MELS,\\n    \"fmin\": F_MIN,\\n    \"fmax\": F_MAX,\\n    \"classes_num\": 10\\n}'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_RATE = configs[\"data\"][\"fs\"]\n",
    "N_FFT = configs[\"feats\"][\"n_window\"]\n",
    "WIN_LENGTH = configs[\"feats\"][\"n_window\"]\n",
    "HOP_LENGTH = configs[\"feats\"][\"hop_length\"]\n",
    "F_MIN = configs[\"feats\"][\"f_min\"]\n",
    "F_MAX = configs[\"feats\"][\"f_max\"]\n",
    "N_MELS = configs[\"feats\"][\"n_mels\"]\n",
    "WINDOW_FN = torch.hamming_window\n",
    "WKWARGS = {\"periodic\": False}\n",
    "POWER = 1\n",
    "NUM_SAMPLES = SAMPLE_RATE\n",
    "\n",
    "LEARNING_RATE = configs[\"opt\"][\"lr\"]\n",
    "epochs = 5\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "    #frame_length_in_seconds\n",
    "frame_length_sec = HOP_LENGTH / SAMPLE_RATE\n",
    "\n",
    "\"\"\"model_config = {\n",
    "    \"sample_rate\": SAMPLE_RATE,\n",
    "    \"window_size\": WIN_LENGTH,\n",
    "    \"hop_size\": HOP_LENGTH,\n",
    "    \"mel_bins\": N_MELS,\n",
    "    \"fmin\": F_MIN,\n",
    "    \"fmax\": F_MAX,\n",
    "    \"classes_num\": 10\n",
    "}\"\"\"\n",
    "\n",
    "#model = PANNsCNN14Att(**model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "classes2id = OrderedDict(\n",
    "    {\n",
    "        \"Alarm_bell_ringing\": 0,\n",
    "        \"Blender\": 1,\n",
    "        \"Cat\": 2,\n",
    "        \"Dishes\": 3,\n",
    "        \"Dog\": 4,\n",
    "        \"Electric_shaver_toothbrush\": 5,\n",
    "        \"Frying\": 6,\n",
    "        \"Running_water\": 7,\n",
    "        \"Speech\": 8,\n",
    "        \"Vacuum_cleaner\": 9,\n",
    "    }\n",
    ")\n",
    "\n",
    "id2classes = {value: key for key, value in classes2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERIOD = 5\n",
    "\n",
    "class PANNsDataset(Dataset):\n",
    "    def __init__(self, annotations_file, audio_dir, \n",
    "                 transformation, target_sample_rate,\n",
    "                 num_samples, device):\n",
    "        self.annotations = pd.read_csv(annotations_file ,sep = \"\\t\")\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.waveform_transforms = False\n",
    "        #self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        audio_sample_path = self._get_audio_sample_path(idx)\n",
    "        label = self._get_audio_sample_label(idx)\n",
    "        label_int = classes2id[label]\n",
    "        onset = self._get_audio_onset_time(idx)\n",
    "        offset = self._get_audio_offset_time(idx)\n",
    "\n",
    "        #waveform, sr = sf.read(audio_sample_path)\n",
    "        #waveform = torch.from_numpy(waveform)\n",
    "        waveform, sr = torchaudio.load(audio_sample_path)\n",
    "        \n",
    "        #print(waveform.shape)\n",
    "        #print(\"****************************\")\n",
    "        #print(waveform.shape)\n",
    "        #waveform = waveform.to(self.device)\n",
    "\n",
    "        waveform = self._resample_if_necessary(waveform, sr)\n",
    "        #print(waveform.shape)\n",
    "        #print(\"&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\")\n",
    "        #make the signal.shape = (1,...)\n",
    "        waveform = self._mix_down_if_necessary(waveform)\n",
    "        #print(waveform.shape)\n",
    "        #print(\"-----------------------------------\")\n",
    "        if waveform.shape[1] > self.num_samples:\n",
    "            waveform = self._cut_if_necessary(waveform, onset, offset)\n",
    "           \n",
    "        waveform = self._right_pad_if_necessary(waveform)\n",
    "        waveform = waveform.view(-1)\n",
    "\n",
    "\n",
    "        if self.waveform_transforms:\n",
    "            y = self.waveform_transforms(y)\n",
    "        else:\n",
    "            \"\"\"len_y = len(y)\n",
    "            effective_length = sr * PERIOD\n",
    "            if len_y < effective_length:\n",
    "                new_y = np.zeros(effective_length, dtype=y.dtype)\n",
    "                start = np.random.randint(effective_length - len_y)\n",
    "                new_y[start:start + len_y] = y\n",
    "                y = new_y.astype(np.float32)\n",
    "            elif len_y > effective_length:\n",
    "                start = np.random.randint(len_y - effective_length)\n",
    "                y = y[start:start + effective_length].astype(np.float32)\n",
    "            else:\n",
    "                y = y.astype(np.float32)\"\"\"\n",
    "\n",
    "        labels = np.zeros(len(classes2id), dtype=\"f\")\n",
    "        labels[classes2id[label]] = 1\n",
    "        \n",
    "          # Convert numpy array to Tensor\n",
    "        #waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        #print(waveform.shape)\n",
    "        #print(\"-----------------------\") \n",
    "        ##print(waveform.shape)\n",
    "         \n",
    "        #print(waveform.size())\n",
    "        #print(type(waveform))\n",
    "        #print(\"######################################\")\n",
    "        #waveform_np = waveform.numpy()\n",
    "        #print(waveform_np.shape)\n",
    "        #print(waveform_np.size)\n",
    "        \n",
    "\n",
    "        return {\"waveform\": waveform, \"targets\": labels}\n",
    "    \n",
    "    def _cut_if_necessary(self, signal, onset, offset):\n",
    "\n",
    "        onset_frame = int(onset * self.target_sample_rate)\n",
    "        offset_frame = int(offset * self.target_sample_rate)\n",
    "        signal = signal[:, onset_frame:offset_frame]\n",
    "\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "    \n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "\n",
    "    \n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "        if not sr == self.target_sample_rate:\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "    \n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim = 0, keepdim = True)\n",
    "        return signal\n",
    "\n",
    "    \n",
    "    def _get_audio_sample_path(self, index):\n",
    "        path = os.path.join(self.audio_dir, self.annotations.iloc[index, 0])\n",
    "        #print(path)\n",
    "        return path\n",
    "    \n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 3]\n",
    "    \n",
    "    def _get_audio_onset_time(self, index):\n",
    "        return self.annotations.iloc[index, 1]\n",
    "    \n",
    "    def _get_audio_offset_time(self, index):\n",
    "        return self.annotations.iloc[index, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PANNsLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bce = nn.BCELoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input_ = input[\"clipwise_output\"]\n",
    "        input_ = torch.where(torch.isnan(input_),\n",
    "                             torch.zeros_like(input_),\n",
    "                             input_)\n",
    "        input_ = torch.where(torch.isinf(input_),\n",
    "                             torch.zeros_like(input_),\n",
    "                             input_)\n",
    "\n",
    "        target = target.float()\n",
    "\n",
    "        return self.bce(input_, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Callback(Callback):\n",
    "    def __init__(self,\n",
    "                 input_key: str = \"targets\",\n",
    "                 output_key: str = \"logits\",\n",
    "                 model_output_key: str = \"clipwise_output\",\n",
    "                 prefix: str = \"f1\"):\n",
    "        super().__init__(CallbackOrder.Metric)\n",
    "\n",
    "        self.input_key = input_key\n",
    "        self.output_key = output_key\n",
    "        self.model_output_key = model_output_key\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def on_loader_start(self, state: Runner):\n",
    "        self.prediction: List[np.ndarray] = []\n",
    "        self.target: List[np.ndarray] = []\n",
    "\n",
    "    def on_batch_end(self, state: Runner):\n",
    "        #print(state)\n",
    "        attributes = dir(state)\n",
    "        #print(attributes)\n",
    "        targ = state.input[self.input_key].detach().cpu().numpy()\n",
    "        out = state.output[self.output_key]\n",
    "\n",
    "        clipwise_output = out[self.model_output_key].detach().cpu().numpy()\n",
    "\n",
    "        self.prediction.append(clipwise_output)\n",
    "        self.target.append(targ)\n",
    "\n",
    "        y_pred = clipwise_output.argmax(axis=1)\n",
    "        y_true = targ.argmax(axis=1)\n",
    "\n",
    "        score = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        state.batch_metrics[self.prefix] = score\n",
    "\n",
    "    def on_loader_end(self, state: Runner):\n",
    "        y_pred = np.concatenate(self.prediction, axis=0).argmax(axis=1)\n",
    "        y_true = np.concatenate(self.target, axis=0).argmax(axis=1)\n",
    "        score = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        state.loader_metrics[self.prefix] = score\n",
    "        if state.is_valid_loader:\n",
    "            state.epoch_metrics[state.valid_loader + \"_epoch_\" +\n",
    "                                self.prefix] = score\n",
    "        else:\n",
    "            state.epoch_metrics[\"train_epoch_\" + self.prefix] = score\n",
    "\n",
    "\n",
    "class mAPCallback(Callback):\n",
    "    def __init__(self,\n",
    "                 input_key: str = \"targets\",\n",
    "                 output_key: str = \"logits\",\n",
    "                 model_output_key: str = \"clipwise_output\",\n",
    "                 prefix: str = \"mAP\"):\n",
    "        super().__init__(CallbackOrder.Metric)\n",
    "        self.input_key = input_key\n",
    "        self.output_key = output_key\n",
    "        self.model_output_key = model_output_key\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def on_loader_start(self, state: Runner):\n",
    "        self.prediction: List[np.ndarray] = []\n",
    "        self.target: List[np.ndarray] = []\n",
    "\n",
    "    def on_batch_end(self, state: Runner):\n",
    "        #print(state)\n",
    "        attributes = dir(state)\n",
    "        #print(attributes)\n",
    "        targ = state.input[self.input_key].detach().cpu().numpy()\n",
    "        out = state.output[self.output_key]\n",
    "\n",
    "        clipwise_output = out[self.model_output_key].detach().cpu().numpy()\n",
    "\n",
    "        self.prediction.append(clipwise_output)\n",
    "        self.target.append(targ)\n",
    "\n",
    "        score = average_precision_score(targ, clipwise_output, average=None)\n",
    "        score = np.nan_to_num(score).mean()\n",
    "        state.batch_metrics[self.prefix] = score\n",
    "\n",
    "    def on_loader_end(self, state: Runner):\n",
    "        y_pred = np.concatenate(self.prediction, axis=0)\n",
    "        y_true = np.concatenate(self.target, axis=0)\n",
    "        score = average_precision_score(y_true, y_pred, average=None)\n",
    "        score = np.nan_to_num(score).mean()\n",
    "        state.loader_metrics[self.prefix] = score\n",
    "        if state.is_valid_loader:\n",
    "            state.epoch_metrics[state.valid_loader + \"_epoch_\" +\n",
    "                                self.prefix] = score\n",
    "        else:\n",
    "            state.epoch_metrics[\"train_epoch_\" + self.prefix] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_dataset = PANNsDataset(annotations_file = configs[\"data\"][\"synth_tsv\"], \n",
    "                                          audio_dir = configs[\"data\"][\"synth_folder\"], \n",
    "                                          transformation = None, \n",
    "                                          target_sample_rate = SAMPLE_RATE,\n",
    "                                          num_samples = NUM_SAMPLES,\n",
    "                                          device = device)\n",
    "\n",
    "val_dataset = PANNsDataset(annotations_file = configs[\"data\"][\"synth_val_tsv\"],\n",
    "                                          audio_dir = configs[\"data\"][\"synth_val_folder\"],\n",
    "                                          transformation = None, \n",
    "                                          target_sample_rate = SAMPLE_RATE,\n",
    "                                          num_samples = NUM_SAMPLES,\n",
    "                                          device = device)\n",
    "# loaders\n",
    "loaders = {\n",
    "    \"train\": DataLoader(train_dataset, \n",
    "                             batch_size= 2, \n",
    "                             shuffle=False),\n",
    "    \"valid\": DataLoader(val_dataset, \n",
    "                             batch_size=2, \n",
    "                             shuffle=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"waveform\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"sample_rate\": 16000,\n",
    "    \"window_size\": 1024,\n",
    "    \"hop_size\": 320,\n",
    "    \"mel_bins\": 64,\n",
    "    \"fmin\": 50,\n",
    "    \"fmax\": 14000,\n",
    "    \"classes_num\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"./log_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1044620/3599870245.py:44: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
      "/home/unegi/anaconda3/envs/sed_starter/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model_config[\"classes_num\"] = 527\n",
    "model = PANNsCNN14Att(**model_config)\n",
    "#weights = torch.load(\"Cnn14_DecisionLevelAtt_mAP0.425.pth\", map_location = \"cpu\")\n",
    "# Fixed in V3\n",
    "#model.load_state_dict(weights[\"model\"])\n",
    "model.att_block = AttBlock(2048, 10, activation='sigmoid')\n",
    "#model.att_block.init_weights()\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# Loss\n",
    "criterion = PANNsLoss().to(device)\n",
    "#F1Callback(input_key=\"targets\", output_key=\"logits\", prefix=\"f1\")\n",
    "# callbacks\n",
    "callbacks = [\n",
    "    \n",
    "    mAPCallback(input_key=\"targets\", output_key=\"logits\", prefix=\"mAP\"),\n",
    "    CheckpointCallback(save_best =0, logdir = logdir)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16000])\n"
     ]
    }
   ],
   "source": [
    "for batch in loaders[\"train\"]:\n",
    "    print(batch[\"waveform\"].shape)    #print(b)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _lwlrap_sklearn(truth, scores):\n",
    "    \"\"\"Reference implementation from https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8\"\"\"\n",
    "    sample_weight = np.sum(truth > 0, axis=1)\n",
    "    nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n",
    "    overall_lwlrap = metrics.label_ranking_average_precision_score(\n",
    "        truth[nonzero_weight_sample_indices, :] > 0, \n",
    "        scores[nonzero_weight_sample_indices, :], \n",
    "        sample_weight=sample_weight[nonzero_weight_sample_indices])\n",
    "    return overall_lwlrap\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "class MetricMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.y_true = []\n",
    "        self.y_pred = []\n",
    "    \n",
    "    def update(self, y_true, y_pred):\n",
    "        self.y_true.extend(y_true.cpu().detach().numpy().tolist())\n",
    "        self.y_pred.extend(y_pred.cpu().detach().numpy().tolist())\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        #score_class, weight = lwlrap(np.array(self.y_true), np.array(self.y_pred))\n",
    "        self.score = _lwlrap_sklearn(np.array(self.y_true), np.array(self.y_pred)) #(score_class * weight).sum()\n",
    "        return {\n",
    "            \"lwlrap\" : self.score\n",
    "        }\n",
    "\n",
    "def seed_everithing(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lwlrap = -np.inf\n",
    "early_stop_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(device, model, loader, criterion, optimizer, scheduler, epoch):\n",
    "    losses = AverageMeter()\n",
    "    scores = MetricMeter()\n",
    "\n",
    "    model.train()\n",
    "    t = tqdm(loader)\n",
    "    for i, sample in enumerate(t):\n",
    "        optimizer.zero_grad()\n",
    "        input = sample['waveform'].to(device)\n",
    "        target = sample['targets'].to(device)\n",
    "        output = model(input)\n",
    "        #print(output)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #if scheduler and args.step_scheduler:\n",
    "            #scheduler.step()\n",
    "\n",
    "        bs = input.size(0)\n",
    "        scores.update(target, torch.sigmoid(torch.max(output['framewise_output'], dim=1)[0]))\n",
    "        losses.update(loss.item(), bs)\n",
    "\n",
    "        t.set_description(f\"Train E:{epoch} - Loss{losses.avg:0.4f}\")\n",
    "    t.close()\n",
    "    return scores.avg, losses.avg\n",
    "        \n",
    "def valid_epoch(device, model, loader, criterion, epoch):\n",
    "    losses = AverageMeter()\n",
    "    scores = MetricMeter()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        t = tqdm(loader)\n",
    "        for i, sample in enumerate(t):\n",
    "            input = sample['waveform'].to(device)\n",
    "            target = sample['targets'].to(device)\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            bs = input.size(0)\n",
    "            scores.update(target, torch.sigmoid(torch.max(output['framewise_output'], dim=1)[0]))\n",
    "            losses.update(loss.item(), bs)\n",
    "            t.set_description(f\"Valid E:{epoch} - Loss:{losses.avg:0.4f}\")\n",
    "    t.close()\n",
    "    return scores.avg, losses.avg\n",
    "\n",
    "def test_epoch(device, model, loader):\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    id_list = []\n",
    "    with torch.no_grad():\n",
    "        t = tqdm(loader)\n",
    "        for i, sample in enumerate(t):\n",
    "            input = sample[\"image\"].to(device)\n",
    "            bs, seq, w = input.shape\n",
    "            input = input.reshape(bs*seq, w)\n",
    "            id = sample[\"id\"]\n",
    "            output = model(input)\n",
    "            output = torch.sigmoid(torch.max(output['framewise_output'], dim=1)[0])\n",
    "            output = output.reshape(bs, seq, -1)\n",
    "            output = torch.sum(output, dim=1)\n",
    "            #output, _ = torch.max(output, dim=1)\n",
    "            output = output.cpu().detach().numpy().tolist()\n",
    "            pred_list.extend(output)\n",
    "            id_list.extend(id)\n",
    "    \n",
    "    return pred_list, id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = PANNsLoss()\n",
    "best_lwlrap = -np.inf\n",
    "early_stop_count = 0\n",
    "scheduler = None\n",
    "save_path = \"./log_dir\"\n",
    "exp_name = \"logging\"\n",
    "early_stop = 15\n",
    "\n",
    "for epoch in range(2):\n",
    "        train_avg, train_loss = train_epoch(device, model, loaders[\"train\"], criterion, optimizer, scheduler, epoch)\n",
    "        valid_avg, valid_loss = valid_epoch(device, model, loaders[\"valid\"], criterion, epoch)\n",
    "        \n",
    "        #if args.epoch_scheduler:\n",
    "           # scheduler.step()\n",
    "        \n",
    "        content = f\"\"\"\n",
    "                {time.ctime()} \\n\n",
    "                Epoch:{epoch}, lr:{optimizer.param_groups[0]['lr']:.7}\\n\n",
    "                Train Loss:{train_loss:0.4f} - LWLRAP:{train_avg['lwlrap']:0.4f}\\n\n",
    "                Valid Loss:{valid_loss:0.4f} - LWLRAP:{valid_avg['lwlrap']:0.4f}\\n\n",
    "        \"\"\"\n",
    "        with open(f'{save_path}/log_{exp_name}.txt', 'a') as appender:\n",
    "            appender.write(content+'\\n')\n",
    "        \n",
    "        if valid_avg['lwlrap'] > best_lwlrap:\n",
    "            print(f\"########## >>>>>>>> Model Improved From {best_lwlrap} ----> {valid_avg['lwlrap']}\")\n",
    "            torch.save(model.state_dict(), save_path+'.bin')\n",
    "            best_lwlrap = valid_avg['lwlrap']\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "        #torch.save(model.state_dict(), os.path.join(args.save_path, f'fold-{args.fold}_last.bin'))\n",
    "\n",
    "        if early_stop == early_stop_count:\n",
    "            print(\"\\n $$$ ---? Ohoo.... we reached early stoping count :\", early_stop_count)\n",
    "            break\n",
    "    \n",
    "model.load_state_dict(torch.load(save_path+'.bin'), map_location=device)\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"target_cols = sub_df.columns[1:].values.tolist()\n",
    "test_pred, ids = test_epoch(device, model, test_loader)\n",
    "print(np.array(test_pred).shape)\n",
    "\n",
    "test_pred_df = pd.DataFrame({\n",
    "        \"recording_id\" : sub_df.recording_id.values\n",
    "    })\n",
    "test_pred_df[target_cols] = test_pred\n",
    "test_pred_df.to_csv(save_path+'.bin'+\"-submission.csv\", index=False)\n",
    "print(os.path.join(save_path, f\"-submission.csv\"))\n",
    "        \n",
    "        #print(content)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = sub_df.columns[1:].values.tolist()\n",
    "test_pred, ids = test_epoch(device, model, test_loader)\n",
    "print(np.array(test_pred).shape)\n",
    "\n",
    "test_pred_df = pd.DataFrame({\n",
    "        \"recording_id\" : sub_df.recording_id.values\n",
    "    })\n",
    "test_pred_df[target_cols] = test_pred\n",
    "test_pred_df.to_csv(save_path+'.bin'+\"-submission.csv\", index=False)\n",
    "print(os.path.join(save_path, f\"-submission.csv\"))\n",
    "        \n",
    "        #print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"warnings.simplefilter(\"ignore\")\n",
    "\n",
    "runner = SupervisedRunner(\n",
    "    input_key=\"waveform\",\n",
    "    target_key=\"targets\")\n",
    "\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    loaders=loaders,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    "    logdir=f\"fold0\",\n",
    "    callbacks=callbacks)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sed_starter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
